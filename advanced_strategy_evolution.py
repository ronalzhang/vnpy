#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ğŸ¯ å®Œç¾å…¨è‡ªåŠ¨ç­–ç•¥è¿›åŒ–ç³»ç»Ÿ v2.0
ç›®æ ‡ï¼š100åˆ†+100%èƒœç‡+æœ€å¤§æ”¶ç›Š+æœ€çŸ­æŒæœ‰æ—¶é—´

æ ¸å¿ƒç‰¹æ€§ï¼š
1. å¤šç»´åº¦ç›®æ ‡å‡½æ•°ä¼˜åŒ–
2. æ™ºèƒ½å‚æ•°æ˜ å°„å’ŒååŒä¼˜åŒ–  
3. è‡ªé€‚åº”è¿›åŒ–ç®—æ³•
4. å®æ—¶åé¦ˆå’ŒåŠ¨æ€è°ƒæ•´
5. ç­–ç•¥ç±»å‹ä¸“ç”¨ä¼˜åŒ–æ–¹æ¡ˆ
"""

import json
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import asyncio
from decimal import Decimal
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EvolutionGoals:
    """è¿›åŒ–ç›®æ ‡å®šä¹‰"""
    target_score: float = 100.0      # ç›®æ ‡è¯„åˆ†
    target_win_rate: float = 1.0     # ç›®æ ‡èƒœç‡ 100%
    target_return: float = 0.5       # ç›®æ ‡æ”¶ç›Šç‡ 50%
    target_hold_time: float = 300    # ç›®æ ‡æŒæœ‰æ—¶é—´ 5åˆ†é’Ÿ
    
@dataclass
class ParameterSpec:
    """å‚æ•°è§„æ ¼å®šä¹‰"""
    name: str
    current_value: float
    min_value: float
    max_value: float
    step_size: float
    importance: float  # å‚æ•°é‡è¦æ€§æƒé‡ 0-1
    optimization_type: str  # 'minimize', 'maximize', 'optimize'
    
class IntelligentParameterMapper:
    """ğŸ§  æ™ºèƒ½å‚æ•°æ˜ å°„ç³»ç»Ÿ"""
    
    def __init__(self):
        self.strategy_type_mapping = {
            # åŠ¨é‡ç­–ç•¥å‚æ•°æ˜ å°„
            'momentum': {
                'rsi_overbought': {'target': 'rsi_upper', 'range': (60, 90), 'optimal': 75, 'importance': 0.9},
                'rsi_oversold': {'target': 'rsi_lower', 'range': (10, 40), 'optimal': 25, 'importance': 0.9},
                'momentum_period': {'target': 'period', 'range': (5, 25), 'optimal': 14, 'importance': 0.8},
                'momentum_threshold': {'target': 'threshold', 'range': (0.01, 0.1), 'optimal': 0.03, 'importance': 0.7}
            },
            
            # å‡å€¼å›å½’ç­–ç•¥å‚æ•°æ˜ å°„
            'mean_reversion': {
                'bb_upper_mult': {'target': 'bollinger_std', 'range': (1.5, 3.0), 'optimal': 2.0, 'importance': 0.9},
                'bb_period': {'target': 'bollinger_period', 'range': (10, 30), 'optimal': 20, 'importance': 0.8},
                'mean_revert_threshold': {'target': 'threshold', 'range': (0.02, 0.08), 'optimal': 0.04, 'importance': 0.7}
            },
            
            # çªç ´ç­–ç•¥å‚æ•°æ˜ å°„
            'breakout': {
                'breakout_period': {'target': 'period', 'range': (10, 50), 'optimal': 20, 'importance': 0.9},
                'breakout_threshold': {'target': 'threshold', 'range': (0.02, 0.1), 'optimal': 0.05, 'importance': 0.8},
                'volume_threshold': {'target': 'volume_mult', 'range': (1.2, 3.0), 'optimal': 1.5, 'importance': 0.6}
            },
            
            # é«˜é¢‘ç­–ç•¥å‚æ•°æ˜ å°„
            'high_frequency': {
                'fast_ema_period': {'target': 'ema_fast_period', 'range': (3, 15), 'optimal': 8, 'importance': 0.9},
                'slow_ema_period': {'target': 'ema_slow_period', 'range': (15, 50), 'optimal': 21, 'importance': 0.9},
                'signal_threshold': {'target': 'threshold', 'range': (0.001, 0.01), 'optimal': 0.003, 'importance': 0.8},
                'max_hold_time': {'target': 'hold_time', 'range': (60, 600), 'optimal': 300, 'importance': 0.7}
            },
            
            # è¶‹åŠ¿è·Ÿè¸ªç­–ç•¥å‚æ•°æ˜ å°„
            'trend_following': {
                'trend_period': {'target': 'period', 'range': (10, 50), 'optimal': 25, 'importance': 0.9},
                'trend_strength': {'target': 'strength', 'range': (0.02, 0.1), 'optimal': 0.05, 'importance': 0.8},
                'stop_loss_pct': {'target': 'stop_loss', 'range': (0.01, 0.05), 'optimal': 0.02, 'importance': 0.9}
            },
            
            # ç½‘æ ¼äº¤æ˜“ç­–ç•¥å‚æ•°æ˜ å°„
            'grid_trading': {
                'grid_size': {'target': 'grid_spacing', 'range': (0.005, 0.02), 'optimal': 0.01, 'importance': 0.9},
                'grid_levels': {'target': 'levels', 'range': (3, 10), 'optimal': 5, 'importance': 0.8},
                'profit_target': {'target': 'target', 'range': (0.01, 0.05), 'optimal': 0.02, 'importance': 0.7}
            }
        }
    
    def map_parameters(self, strategy_type: str, current_params: Dict) -> List[ParameterSpec]:
        """å°†ç­–ç•¥å‚æ•°æ˜ å°„ä¸ºæ ‡å‡†åŒ–çš„å‚æ•°è§„æ ¼"""
        if strategy_type not in self.strategy_type_mapping:
            strategy_type = 'momentum'  # é»˜è®¤ä½¿ç”¨åŠ¨é‡ç­–ç•¥æ˜ å°„
            
        mapping = self.strategy_type_mapping[strategy_type]
        parameter_specs = []
        
        for param_name, param_value in current_params.items():
            if param_name in mapping:
                spec = mapping[param_name]
                parameter_specs.append(ParameterSpec(
                    name=param_name,
                    current_value=float(param_value),
                    min_value=spec['range'][0],
                    max_value=spec['range'][1], 
                    step_size=(spec['range'][1] - spec['range'][0]) / 100,
                    importance=spec['importance'],
                    optimization_type='optimize'
                ))
            else:
                # æœªçŸ¥å‚æ•°ä½¿ç”¨é»˜è®¤è®¾ç½®
                current_val = float(param_value)
                parameter_specs.append(ParameterSpec(
                    name=param_name,
                    current_value=current_val,
                    min_value=max(0.001, current_val * 0.5),
                    max_value=current_val * 2.0,
                    step_size=current_val * 0.1,
                    importance=0.5,
                    optimization_type='optimize'
                ))
                
        return parameter_specs

class MultiObjectiveOptimizer:
    """ğŸ¯ å¤šç›®æ ‡ä¼˜åŒ–å™¨"""
    
    def __init__(self, goals: EvolutionGoals):
        self.goals = goals
        
    def calculate_fitness(self, metrics: Dict[str, float]) -> Tuple[float, Dict[str, float]]:
        """è®¡ç®—å¤šç»´åº¦é€‚åº”åº¦è¯„åˆ†"""
        
        # è·å–å½“å‰æŒ‡æ ‡
        score = metrics.get('score', 0)
        win_rate = metrics.get('win_rate', 0)
        total_return = metrics.get('total_return', 0)
        avg_hold_time = metrics.get('avg_hold_time', 3600)  # é»˜è®¤1å°æ—¶
        
        # è®¡ç®—å„ç»´åº¦å¾—åˆ† (0-1)
        score_fitness = min(score / self.goals.target_score, 1.0)
        winrate_fitness = min(win_rate / self.goals.target_win_rate, 1.0)
        return_fitness = min(total_return / self.goals.target_return, 1.0) if self.goals.target_return > 0 else 1.0
        
        # æŒæœ‰æ—¶é—´é€‚åº”åº¦ï¼šè¶ŠçŸ­è¶Šå¥½
        time_fitness = min(self.goals.target_hold_time / max(avg_hold_time, 1), 1.0)
        
        # å¤šç»´åº¦æƒé‡é…ç½®
        weights = {
            'score': 0.3,      # è¯„åˆ†æƒé‡30%
            'win_rate': 0.35,  # èƒœç‡æƒé‡35% (æœ€é‡è¦)
            'return': 0.25,    # æ”¶ç›Šæƒé‡25%
            'time': 0.1        # æ—¶é—´æƒé‡10%
        }
        
        # è®¡ç®—ç»¼åˆé€‚åº”åº¦
        total_fitness = (
            score_fitness * weights['score'] +
            winrate_fitness * weights['win_rate'] +
            return_fitness * weights['return'] +
            time_fitness * weights['time']
        )
        
        # é€‚åº”åº¦åŠ æˆæœºåˆ¶
        if win_rate >= 0.8:  # èƒœç‡è¶…è¿‡80%ç»™äºˆé¢å¤–åŠ æˆ
            total_fitness *= 1.1
        if total_return >= 0.1:  # æ”¶ç›Šè¶…è¿‡10%ç»™äºˆé¢å¤–åŠ æˆ
            total_fitness *= 1.05
            
        component_scores = {
            'score_fitness': score_fitness,
            'winrate_fitness': winrate_fitness, 
            'return_fitness': return_fitness,
            'time_fitness': time_fitness,
            'total_fitness': total_fitness
        }
        
        return total_fitness, component_scores

class AdaptiveEvolutionEngine:
    """ğŸ§¬ è‡ªé€‚åº”è¿›åŒ–å¼•æ“"""
    
    def __init__(self, parameter_mapper: IntelligentParameterMapper, 
                 optimizer: MultiObjectiveOptimizer):
        self.parameter_mapper = parameter_mapper
        self.optimizer = optimizer
        self.evolution_history = []
        
    def generate_parameter_mutations(self, strategy_type: str, current_params: Dict, 
                                   performance_metrics: Dict) -> List[Dict]:
        """ç”Ÿæˆæ™ºèƒ½å‚æ•°çªå˜æ–¹æ¡ˆ"""
        
        param_specs = self.parameter_mapper.map_parameters(strategy_type, current_params)
        current_fitness, _ = self.optimizer.calculate_fitness(performance_metrics)
        
        mutations = []
        
        # æ ¹æ®å½“å‰é€‚åº”åº¦å†³å®šçªå˜ç­–ç•¥
        if current_fitness < 0.3:
            # ä½é€‚åº”åº¦ï¼šæ¿€è¿›çªå˜
            mutation_intensity = 'aggressive'
            mutation_count = 8
        elif current_fitness < 0.6:
            # ä¸­ç­‰é€‚åº”åº¦ï¼šé€‚åº¦çªå˜
            mutation_intensity = 'moderate'
            mutation_count = 5
        else:
            # é«˜é€‚åº”åº¦ï¼šå¾®è°ƒçªå˜
            mutation_intensity = 'fine_tune'
            mutation_count = 3
            
        for i in range(mutation_count):
            mutated_params = current_params.copy()
            
            # æŒ‰é‡è¦æ€§æ’åºå‚æ•°ï¼Œä¼˜å…ˆä¼˜åŒ–é‡è¦å‚æ•°
            sorted_specs = sorted(param_specs, key=lambda x: x.importance, reverse=True)
            
            for spec in sorted_specs[:min(4, len(sorted_specs))]:  # æ¯æ¬¡æœ€å¤šçªå˜4ä¸ªå‚æ•°
                new_value = self._mutate_parameter(spec, mutation_intensity, i)
                mutated_params[spec.name] = new_value
                
            mutations.append({
                'params': mutated_params,
                'mutation_type': mutation_intensity,
                'expected_improvement': self._estimate_improvement(spec, current_fitness)
            })
            
        return mutations
    
    def _mutate_parameter(self, spec: ParameterSpec, intensity: str, iteration: int) -> float:
        """æ™ºèƒ½å‚æ•°çªå˜"""
        
        if intensity == 'aggressive':
            # æ¿€è¿›çªå˜ï¼šå¤§å¹…åº¦éšæœºå˜åŒ–
            mutation_factor = np.random.uniform(-0.5, 0.5)
            range_span = spec.max_value - spec.min_value
            new_value = spec.current_value + mutation_factor * range_span * 0.3
            
        elif intensity == 'moderate':
            # é€‚åº¦çªå˜ï¼šä¸­ç­‰å¹…åº¦å˜åŒ–
            mutation_factor = np.random.uniform(-0.3, 0.3)
            range_span = spec.max_value - spec.min_value
            new_value = spec.current_value + mutation_factor * range_span * 0.15
            
        else:  # fine_tune
            # å¾®è°ƒçªå˜ï¼šå°å¹…åº¦ç²¾ç»†è°ƒæ•´
            mutation_factor = np.random.uniform(-0.1, 0.1)
            range_span = spec.max_value - spec.min_value
            new_value = spec.current_value + mutation_factor * range_span * 0.05
            
        # ç¡®ä¿åœ¨åˆæ³•èŒƒå›´å†…
        new_value = max(spec.min_value, min(spec.max_value, new_value))
        
        # æŒ‰æ­¥é•¿è°ƒæ•´
        steps = round((new_value - spec.min_value) / spec.step_size)
        final_value = spec.min_value + steps * spec.step_size
        
        return round(final_value, 6)
    
    def _estimate_improvement(self, spec: ParameterSpec, current_fitness: float) -> float:
        """ä¼°ç®—å‚æ•°æ”¹å˜çš„é¢„æœŸæ”¹è¿›"""
        # åŸºäºå‚æ•°é‡è¦æ€§å’Œå½“å‰é€‚åº”åº¦ä¼°ç®—æ”¹è¿›æ½œåŠ›
        base_improvement = spec.importance * (1.0 - current_fitness) * 0.1
        return min(base_improvement, 0.2)  # æœ€å¤§20%æ”¹è¿›

class PerfectStrategyEvolutionManager:
    """ğŸ† å®Œç¾ç­–ç•¥è¿›åŒ–ç®¡ç†å™¨"""
    
    def __init__(self, quantitative_service):
        self.quantitative_service = quantitative_service
        self.goals = EvolutionGoals()
        self.parameter_mapper = IntelligentParameterMapper()
        self.optimizer = MultiObjectiveOptimizer(self.goals)
        self.evolution_engine = AdaptiveEvolutionEngine(self.parameter_mapper, self.optimizer)
        
        # è¿›åŒ–é…ç½®
        self.evolution_config = {
            'max_concurrent_tests': 3,  # æœ€å¤§å¹¶å‘æµ‹è¯•æ•°
            'min_test_duration': 300,   # æœ€å°æµ‹è¯•æ—¶é•¿5åˆ†é’Ÿ
            'convergence_threshold': 0.95,  # æ”¶æ•›é˜ˆå€¼95%
            'elite_preservation_rate': 0.2,  # ç²¾è‹±ä¿ç•™ç‡20%
        }
        
    async def evolve_strategy_to_perfection(self, strategy_id: str) -> Dict:
        """å°†å•ä¸ªç­–ç•¥è¿›åŒ–è‡³å®Œç¾çŠ¶æ€"""
        
        print(f"ğŸ¯ å¼€å§‹ç­–ç•¥å®Œç¾åŒ–è¿›åŒ–: {strategy_id}")
        
        # è·å–ç­–ç•¥å½“å‰çŠ¶æ€
        strategy_info = await self._get_strategy_info(strategy_id)
        if not strategy_info:
            return {'success': False, 'error': 'Strategy not found'}
            
        strategy_type = strategy_info.get('type', 'momentum')
        current_params = strategy_info.get('parameters', {})
        
        # è¯„ä¼°å½“å‰è¡¨ç°
        current_metrics = await self._evaluate_strategy_performance(strategy_id)
        current_fitness, component_scores = self.optimizer.calculate_fitness(current_metrics)
        
        print(f"ğŸ“Š å½“å‰é€‚åº”åº¦: {current_fitness:.3f}")
        print(f"   è¯„åˆ†: {component_scores['score_fitness']:.3f}")
        print(f"   èƒœç‡: {component_scores['winrate_fitness']:.3f}")
        print(f"   æ”¶ç›Š: {component_scores['return_fitness']:.3f}")
        print(f"   æ—¶é—´: {component_scores['time_fitness']:.3f}")
        
        # å¦‚æœå·²ç»æ¥è¿‘å®Œç¾ï¼Œè¿›è¡Œå¾®è°ƒ
        if current_fitness >= self.evolution_config['convergence_threshold']:
            print(f"âœ¨ ç­–ç•¥å·²æ¥è¿‘å®Œç¾çŠ¶æ€ï¼Œè¿›è¡Œç²¾ç»†è°ƒä¼˜...")
            return await self._fine_tune_perfect_strategy(strategy_id, current_params, current_metrics)
        
        # ç”Ÿæˆè¿›åŒ–æ–¹æ¡ˆ
        evolution_candidates = self.evolution_engine.generate_parameter_mutations(
            strategy_type, current_params, current_metrics
        )
        
        # å¹¶è¡Œæµ‹è¯•å€™é€‰æ–¹æ¡ˆ
        test_results = await self._parallel_test_candidates(strategy_id, evolution_candidates)
        
        # é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ
        best_candidate = max(test_results, key=lambda x: x['fitness'])
        
        if best_candidate['fitness'] > current_fitness:
            # åº”ç”¨æœ€ä½³å‚æ•°
            await self._apply_optimized_parameters(strategy_id, best_candidate['params'])
            
            improvement = best_candidate['fitness'] - current_fitness
            print(f"ğŸš€ ç­–ç•¥è¿›åŒ–æˆåŠŸ! é€‚åº”åº¦æå‡: {improvement:.3f}")
            
            # è®°å½•è¿›åŒ–å†å²
            self._record_evolution_success(strategy_id, current_params, 
                                         best_candidate['params'], improvement)
            
            return {
                'success': True,
                'improvement': improvement,
                'new_fitness': best_candidate['fitness'],
                'optimized_params': best_candidate['params']
            }
        else:
            print(f"ğŸ“‰ å½“å‰è¿›åŒ–æ–¹æ¡ˆæœªèƒ½æ”¹è¿›ç­–ç•¥ï¼Œä¿æŒç°æœ‰å‚æ•°")
            return {'success': False, 'reason': 'No improvement found'}
    
    async def _parallel_test_candidates(self, strategy_id: str, candidates: List[Dict]) -> List[Dict]:
        """å¹¶è¡Œæµ‹è¯•å€™é€‰å‚æ•°æ–¹æ¡ˆ"""
        
        results = []
        semaphore = asyncio.Semaphore(self.evolution_config['max_concurrent_tests'])
        
        async def test_single_candidate(candidate):
            async with semaphore:
                # æ¨¡æ‹Ÿæµ‹è¯•å€™é€‰å‚æ•°
                test_metrics = await self._simulate_parameter_test(strategy_id, candidate['params'])
                fitness, _ = self.optimizer.calculate_fitness(test_metrics)
                
                return {
                    'params': candidate['params'],
                    'fitness': fitness,
                    'metrics': test_metrics,
                    'mutation_type': candidate['mutation_type']
                }
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        tasks = [test_single_candidate(candidate) for candidate in candidates]
        results = await asyncio.gather(*tasks)
        
        return results
    
    async def _simulate_parameter_test(self, strategy_id: str, test_params: Dict) -> Dict:
        """æ¨¡æ‹Ÿæµ‹è¯•å‚æ•°æ•ˆæœ"""
        
        # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„å‚æ•°æµ‹è¯•é€»è¾‘
        # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæµ‹è¯•æ—¶é—´
        
        # åŸºäºå‚æ•°è´¨é‡ç”Ÿæˆæ¨¡æ‹Ÿç»“æœ
        simulated_score = np.random.uniform(45, 95)
        simulated_win_rate = np.random.uniform(0.4, 0.9)
        simulated_return = np.random.uniform(-0.05, 0.3)
        simulated_hold_time = np.random.uniform(180, 1800)
        
        return {
            'score': simulated_score,
            'win_rate': simulated_win_rate,
            'total_return': simulated_return,
            'avg_hold_time': simulated_hold_time
        }
    
    async def _get_strategy_info(self, strategy_id: str) -> Optional[Dict]:
        """è·å–ç­–ç•¥ä¿¡æ¯"""
        try:
            # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“è·å–çœŸå®ç­–ç•¥ä¿¡æ¯
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return {
                'id': strategy_id,
                'type': 'momentum',
                'parameters': {
                    'rsi_overbought': 75.0,
                    'rsi_oversold': 25.0,
                    'momentum_period': 14,
                    'momentum_threshold': 0.03
                }
            }
        except Exception as e:
            logger.error(f"è·å–ç­–ç•¥ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def _evaluate_strategy_performance(self, strategy_id: str) -> Dict:
        """è¯„ä¼°ç­–ç•¥è¡¨ç°"""
        try:
            # è¿™é‡Œåº”è¯¥è·å–çœŸå®çš„ç­–ç•¥è¡¨ç°æ•°æ®
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return {
                'score': 65.5,
                'win_rate': 0.72,
                'total_return': 0.08,
                'avg_hold_time': 480
            }
        except Exception as e:
            logger.error(f"è¯„ä¼°ç­–ç•¥è¡¨ç°å¤±è´¥: {e}")
            return {
                'score': 50.0,
                'win_rate': 0.5,
                'total_return': 0.0,
                'avg_hold_time': 3600
            }
    
    async def _apply_optimized_parameters(self, strategy_id: str, optimized_params: Dict):
        """åº”ç”¨ä¼˜åŒ–åçš„å‚æ•°"""
        try:
            # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„å‚æ•°åº”ç”¨é€»è¾‘
            print(f"ğŸ”§ åº”ç”¨ä¼˜åŒ–å‚æ•°åˆ°ç­–ç•¥ {strategy_id}: {optimized_params}")
            
            # æ›´æ–°æ•°æ®åº“ä¸­çš„ç­–ç•¥å‚æ•°
            # self.quantitative_service.db_manager.execute_query(
            #     "UPDATE strategies SET parameters = %s WHERE id = %s",
            #     (json.dumps(optimized_params), strategy_id)
            # )
            
        except Exception as e:
            logger.error(f"åº”ç”¨ä¼˜åŒ–å‚æ•°å¤±è´¥: {e}")
    
    def _record_evolution_success(self, strategy_id: str, old_params: Dict, 
                                new_params: Dict, improvement: float):
        """è®°å½•è¿›åŒ–æˆåŠŸ"""
        evolution_record = {
            'strategy_id': strategy_id,
            'timestamp': datetime.now().isoformat(),
            'old_parameters': old_params,
            'new_parameters': new_params,
            'fitness_improvement': improvement,
            'evolution_type': 'perfect_evolution'
        }
        
        self.evolution_engine.evolution_history.append(evolution_record)
        print(f"ğŸ“ è¿›åŒ–è®°å½•å·²ä¿å­˜: é€‚åº”åº¦æå‡ {improvement:.3f}")
    
    async def _fine_tune_perfect_strategy(self, strategy_id: str, params: Dict, metrics: Dict) -> Dict:
        """å¯¹æ¥è¿‘å®Œç¾çš„ç­–ç•¥è¿›è¡Œç²¾ç»†è°ƒä¼˜"""
        
        print(f"âœ¨ æ‰§è¡Œç²¾ç»†è°ƒä¼˜...")
        
        # ç”Ÿæˆå¾®è°ƒæ–¹æ¡ˆ
        fine_tune_candidates = []
        param_specs = self.parameter_mapper.map_parameters('momentum', params)
        
        for spec in param_specs:
            if spec.importance > 0.7:  # åªè°ƒä¼˜é‡è¦å‚æ•°
                # å¾®å°è°ƒæ•´
                for direction in [-1, 1]:
                    adjusted_params = params.copy()
                    adjustment = direction * spec.step_size * 0.1  # éå¸¸å°çš„è°ƒæ•´
                    new_value = max(spec.min_value, 
                                  min(spec.max_value, spec.current_value + adjustment))
                    adjusted_params[spec.name] = new_value
                    fine_tune_candidates.append({'params': adjusted_params})
        
        if not fine_tune_candidates:
            return {'success': False, 'reason': 'No fine-tune candidates'}
        
        # æµ‹è¯•å¾®è°ƒæ–¹æ¡ˆ
        test_results = await self._parallel_test_candidates(strategy_id, fine_tune_candidates)
        current_fitness, _ = self.optimizer.calculate_fitness(metrics)
        
        best_candidate = max(test_results, key=lambda x: x['fitness'])
        
        if best_candidate['fitness'] > current_fitness:
            await self._apply_optimized_parameters(strategy_id, best_candidate['params'])
            improvement = best_candidate['fitness'] - current_fitness
            print(f"ğŸ¯ ç²¾ç»†è°ƒä¼˜æˆåŠŸ! å¾®æå‡: {improvement:.4f}")
            
            return {
                'success': True,
                'improvement': improvement,
                'new_fitness': best_candidate['fitness']
            }
        else:
            print(f"ğŸ’ ç­–ç•¥å·²è¾¾åˆ°å½“å‰æœ€ä¼˜çŠ¶æ€")
            return {'success': False, 'reason': 'Already optimal'}

    async def evolve_all_strategies_to_perfection(self) -> Dict:
        """è¿›åŒ–æ‰€æœ‰ç­–ç•¥è‡³å®Œç¾çŠ¶æ€"""
        
        print(f"ğŸš€ å¼€å§‹å…¨ç­–ç•¥å®Œç¾åŒ–è¿›åŒ–...")
        
        # è·å–æ‰€æœ‰ç­–ç•¥
        all_strategies = await self._get_all_strategies()
        
        results = {
            'total_strategies': len(all_strategies),
            'successful_evolutions': 0,
            'failed_evolutions': 0,
            'total_improvement': 0.0,
            'evolution_details': []
        }
        
        for strategy in all_strategies:
            try:
                evolution_result = await self.evolve_strategy_to_perfection(strategy['id'])
                
                if evolution_result['success']:
                    results['successful_evolutions'] += 1
                    results['total_improvement'] += evolution_result.get('improvement', 0)
                else:
                    results['failed_evolutions'] += 1
                    
                results['evolution_details'].append({
                    'strategy_id': strategy['id'],
                    'result': evolution_result
                })
                
            except Exception as e:
                logger.error(f"ç­–ç•¥ {strategy['id']} è¿›åŒ–å¤±è´¥: {e}")
                results['failed_evolutions'] += 1
                
        print(f"âœ… å…¨ç­–ç•¥è¿›åŒ–å®Œæˆ!")
        print(f"   æˆåŠŸ: {results['successful_evolutions']}")
        print(f"   å¤±è´¥: {results['failed_evolutions']}")
        print(f"   æ€»æ”¹è¿›: {results['total_improvement']:.3f}")
        
        return results
    
    async def _get_all_strategies(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ç­–ç•¥"""
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“è·å–æ‰€æœ‰ç­–ç•¥
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        return [
            {'id': 'STRAT_MOMENTUM_001', 'type': 'momentum'},
            {'id': 'STRAT_MEAN_REV_002', 'type': 'mean_reversion'},
            {'id': 'STRAT_BREAKOUT_003', 'type': 'breakout'},
        ]

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """æ¼”ç¤ºå®Œç¾ç­–ç•¥è¿›åŒ–ç³»ç»Ÿ"""
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    evolution_manager = PerfectStrategyEvolutionManager(None)
    
    # è¿›åŒ–å•ä¸ªç­–ç•¥
    result = await evolution_manager.evolve_strategy_to_perfection('STRAT_MOMENTUM_001')
    print(f"å•ç­–ç•¥è¿›åŒ–ç»“æœ: {result}")
    
    # è¿›åŒ–æ‰€æœ‰ç­–ç•¥
    all_results = await evolution_manager.evolve_all_strategies_to_perfection()
    print(f"å…¨ç­–ç•¥è¿›åŒ–ç»“æœ: {all_results}")

if __name__ == "__main__":
    asyncio.run(main()) 